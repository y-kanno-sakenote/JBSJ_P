# modules/analysis/coauthor/ui.py
from __future__ import annotations
import pandas as pd
import streamlit as st


from modules.common.state import GlobalFilters
from modules.common import banners, copyui
from .filters_adapter import adapt_filter_bar, augment_with_session_state, split_authors
from .compute import (author_total_counts, yearly_author_counts,
                      build_coauthor_edges, centrality_from_edges)
from .network_view import draw_network

# --- Optional deps / utilities for network summary & caching ---
try:
    import networkx as nx  # type: ignore
    HAS_NX = True
except Exception:
    HAS_NX = False

try:
    from modules.common.cache_utils import cache_csv_path, load_csv_if_exists, save_csv  # type: ignore
    HAS_DISK_CACHE = True
except Exception:
    HAS_DISK_CACHE = False

# Optional: Japanese reading (ã‚ˆã¿) for author labels
try:
    from pykakasi import kakasi  # type: ignore
    _KKS = kakasi(); _KKS.setMode('J','H'); _KKS.setMode('K','H'); _KKS.setMode('H','H')
    HAS_KAKASI = True
except Exception:
    HAS_KAKASI = False
    _KKS = None  # type: ignore

# Fallback: optionally load precomputed readings from data/authors_readings.csv
_AUTHOR_READINGS: dict | None = None

def _ensure_author_readings() -> None:
    """Lazy-load a CSV file with author reading mappings (name -> reading).

    Tries a couple of likely locations (project ./data/ and package-relative). No exception
    is raised; failures silently leave the mapping empty.
    """
    global _AUTHOR_READINGS
    if _AUTHOR_READINGS is not None:
        return
    _AUTHOR_READINGS = {}
    try:
        from pathlib import Path
        import pandas as _pd
        cand = [Path.cwd() / "data" / "authors_readings.csv", Path(__file__).resolve().parents[3] / "data" / "authors_readings.csv"]
        for p in cand:
            if p.exists():
                try:
                    df = _pd.read_csv(p, encoding="utf-8")
                    cols = [c for c in df.columns]
                    if len(cols) >= 2:
                        key_col, val_col = cols[0], cols[1]
                        for _, r in df.iterrows():
                            name = str(r.get(key_col, "")).strip()
                            yomi = str(r.get(val_col, "")).strip()
                            if name:
                                _AUTHOR_READINGS[name] = yomi
                    break
                except Exception:
                    # ignore and try next candidate
                    pass
    except Exception:
        _AUTHOR_READINGS = {}

# Shared palette (sync with network colors if needed)
_PALETTE = [
    "#4c78a8", "#f58518", "#54a24b", "#e45756", "#72b7b2",
    "#b279a2", "#ff9da6", "#9d755d", "#bab0ac", "#8c6d31"
]

def _author_label(name: str) -> str:
    """æ¼¢å­—ï½œã‚ˆã¿ï¼ˆpykakasiãŒã‚ã‚Œã°ï¼‰"""
    if HAS_KAKASI and _KKS is not None:
        try:
            yomi = _KKS.getConverter().do(str(name))
            if yomi:
                return f"{name}ï½œ{yomi}"
        except Exception:
            pass
    # fallback: precomputed readings file (ãƒ‡ãƒ—ãƒ­ã‚¤ç’°å¢ƒã§ pykakasi æœªå°å…¥ã®å ´åˆã®è£œåŠ©)
    try:
        _ensure_author_readings()
        if _AUTHOR_READINGS and name in _AUTHOR_READINGS and _AUTHOR_READINGS[name]:
            return f"{name}ï½œ{_AUTHOR_READINGS[name]}"
    except Exception:
        pass
    return str(name)

def _color_square_data_uri(hex_color: str, size: int = 16) -> str:
    """Small colored square as SVG data URI (no Pillow dependency)."""
    import base64
    svg = (
        f"<svg xmlns='http://www.w3.org/2000/svg' width='{size}' height='{size}'>"
        f"<rect width='{size}' height='{size}' fill='{hex_color}' rx='3' ry='3'/></svg>"
    )
    b64 = base64.b64encode(svg.encode("utf-8")).decode("ascii")
    return f"data:image/svg+xml;base64,{b64}"

_HEADER_HTML = """
<div style="display:flex; align-items:center; gap:10px; flex-wrap:wrap; margin: 0 0 4px 0;">
  <h2 style="margin:0; line-height:1; font-weight:600;">ğŸ‘¨â€ğŸ”¬ ç ”ç©¶è€…</h2>
  <div style="margin:0 0 2px 0; line-height:1.2; opacity:0.8; font-size:0.95rem;">
    è‘—è€…åˆ¥ã®è«–æ–‡æ•°ãƒ»å…±è‘—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ç¢ºèªã§ãã¾ã™ã€‚
  </div>
</div>
"""

_METRIC_JA = {
    "degree": "æ¬¡æ•°ï¼ˆã¤ãªãŒã‚Šã®æ•°ï¼‰",
    "betweenness": "åª’ä»‹ï¼ˆæ©‹æ¸¡ã—åº¦ï¼‰",
    "eigenvector": "å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå½±éŸ¿åŠ›ï¼‰",
}

def _summarize(y_from: int, y_to: int, tg_sel, tp_sel) -> str:
    gf = GlobalFilters(y_from, y_to, tg_sel, tp_sel)
    return banners.summarize(gf)

def render_coauthor_tab(df: pd.DataFrame, use_disk_cache: bool = False):
    st.markdown(_HEADER_HTML, unsafe_allow_html=True)
    if df is None or ("è‘—è€…" not in df.columns):
        st.warning("è‘—è€…ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    df_use, y_from, y_to, tg_sel, tp_sel = adapt_filter_bar(df)
    y_from, y_to, tg_sel, tp_sel = augment_with_session_state(y_from, y_to, tg_sel, tp_sel, key_prefix="authors")
    banners.render_provenance(df_use, len(df), GlobalFilters(y_from, y_to, tg_sel, tp_sel))

    tab_count, tab_network, tab_trend = st.tabs(["â‘  è«–æ–‡æ•°", "â‘¡ å…±è‘—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"])

    # ===== â‘  è«–æ–‡æ•° =====
    with tab_count:
        c1, c2, c3, c4 = st.columns([2, 2, 2, 1])
        with c1:
            mode = st.radio("è‘—è€…æ•°ãƒ•ã‚£ãƒ«ã‚¿", ["ã™ã¹ã¦", "å˜è‘—ã®ã¿", "å…±è‘—ã®ã¿"], horizontal=True, key="res_cnt_mode")
        with c2:
            period = st.radio("é›†è¨ˆæœŸé–“", ["ç´¯è¨ˆ", "ç›´è¿‘1å¹´", "ç›´è¿‘3å¹´", "ç›´è¿‘5å¹´"], horizontal=True, key="res_cnt_period")
        with c3:
            position = st.multiselect("è‘—è€…ãƒã‚¸ã‚·ãƒ§ãƒ³", ["ç­†é ­ã®ã¿","è²¬ä»»è‘—è€…ã®ã¿"], key="res_cnt_position")
        with c4:
            top_n = st.number_input("ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä»¶æ•°", min_value=5, max_value=200, value=50, step=5, key="res_cnt_topn")

        df_rank = df_use
        if period != "ç´¯è¨ˆ" and "ç™ºè¡Œå¹´" in df_rank.columns:
            years = pd.to_numeric(df_rank["ç™ºè¡Œå¹´"], errors="coerce")
            span = {"ç›´è¿‘1å¹´":1, "ç›´è¿‘3å¹´":3, "ç›´è¿‘5å¹´":5}[period]
            y_max = int(years.max()) if years.notna().any() else None
            if y_max is not None:
                df_rank = df_rank[(years >= y_max - span + 1) & (years <= y_max)]

        if mode != "ã™ã¹ã¦":
            df_rank = df_rank.copy()
            df_rank["è‘—è€…æ•°"] = df_rank["è‘—è€…"].fillna("").map(lambda s: len(split_authors(s)))
            if mode == "å˜è‘—ã®ã¿":
                df_rank = df_rank[df_rank["è‘—è€…æ•°"] == 1]
            else:
                df_rank = df_rank[df_rank["è‘—è€…æ•°"] >= 2]

        if position:
            bags = []
            for _, r in df_rank.iterrows():
                names = list(dict.fromkeys(split_authors(r.get("è‘—è€…", ""))))
                if not names:
                    continue
                chosen = []
                if "ç­†é ­ã®ã¿" in position and len(names) >= 1:
                    chosen.append(names[0])
                if "è²¬ä»»è‘—è€…ã®ã¿" in position and len(names) >= 1:
                    chosen.append(names[-1])
                if chosen:
                    bags.extend(list(dict.fromkeys(chosen)))
            s = pd.Series(bags, dtype="object").value_counts().sort_values(ascending=False) if bags else pd.Series(dtype=int)
        else:
            s = author_total_counts(df_rank)

        if s.empty:
            st.info("æ¡ä»¶ã«åˆã†ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            rank = s.reset_index()
            rank.columns = ["è‘—è€…", "è«–æ–‡æ•°"]
            rank = rank.sort_values(["è«–æ–‡æ•°", "è‘—è€…"], ascending=[False, True])
            rank_shown = rank.head(int(top_n))

            left, right = st.columns([1.0, 1.6])
            with left:
                st.dataframe(rank_shown[["è‘—è€…", "è«–æ–‡æ•°"]], use_container_width=True, hide_index=True, height=420)
            with right:
                try:
                    import plotly.express as px
                    bar_df = rank.head(10).sort_values("è«–æ–‡æ•°", ascending=False)
                    fig = px.bar(bar_df, x="è«–æ–‡æ•°", y="è‘—è€…", orientation="h", text_auto=True, title="è‘—è€…Top10")
                    fig.update_layout(margin=dict(l=6, r=6, t=40, b=6), height=420, xaxis_title=None, yaxis_title=None)
                    fig.update_yaxes(autorange="reversed")
                    st.plotly_chart(fig, use_container_width=True)
                except Exception:
                    st.bar_chart(rank.set_index("è‘—è€…")["è«–æ–‡æ•°"].head(10))

            parts = []
            if mode != "ã™ã¹ã¦": parts.append(mode)
            if position: parts.append("ãƒ»".join(position))
            parts.append(period)
            st.caption(f"æ¡ä»¶ï¼š{'ãƒ»'.join(parts)} ï½œ ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä»¶æ•°ï¼š{int(top_n)} ï½œ " + _summarize(y_from, y_to, tg_sel, tp_sel))


    # ===== â‘¡ å…±è‘—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ =====
    with tab_network:
        # ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆç”¨ï¼šè‘—è€…å€™è£œï¼ˆé »åº¦ä¸Šä½ã‹ã‚‰æœ€å¤§600ä»¶ï¼‰
        try:
            _auth_freq = author_total_counts(df_use)
            _author_names = _auth_freq.index.tolist()[:600]
        except Exception:
            _bags = []
            for a in df_use.get("è‘—è€…", pd.Series(dtype=str)).fillna(""):
                _bags.extend(split_authors(a))
            _author_names = sorted(list(dict.fromkeys(_bags)))[:600]

        # è¡¨ç¤ºã¯ã€Œæ¼¢å­—ï½œã‚ˆã¿ã€ã€‚æ¤œç´¢ã¯èª­ã¿ã§ã‚‚å¯ï¼ˆStreamlitã¯è¡¨ç¤ºæ–‡å­—åˆ—ã§æ¤œç´¢ï¼‰
        _author_labels = [_author_label(n) for n in _author_names]
        _label_to_name = {lbl: nm for lbl, nm in zip(_author_labels, _author_names)}

        # ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä»¶æ•°ãƒ»æœ€å°å…±è‘—å›æ•°ãƒ»å¿…é ˆãƒ»é™¤å¤–
        c4, c5, c6, c7, c8 = st.columns([1,1,1,2,2])
        with c4:
            metric = st.selectbox(
                "ä¸­å¿ƒæ€§æŒ‡æ¨™",
                ["degree", "betweenness", "eigenvector"],
                index=0,
                format_func=lambda x: _METRIC_JA.get(x, x),
                help="networkx ãŒæœªå°å…¥ã®å ´åˆã¯ç°¡æ˜“ã‚¹ã‚³ã‚¢ï¼ˆå…±è‘—æ•°ã®åˆè¨ˆï¼‰ã§ä»£æ›¿ã—ã¾ã™ã€‚",
                key="res_net_metric",
            )
        with c5:
            top_n = st.number_input(
                "ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä»¶æ•°",
                min_value=5, max_value=100, value=30, step=5,
                key="res_net_topn",
            )
        with c6:
            min_w = st.number_input(
                "æœ€å°å…±è‘—å›æ•°ï¼ˆw â‰¥ï¼‰",
                min_value=1, max_value=20, value=2, step=1,
                key="res_net_minw",
                help="ã“ã®å›æ•°æœªæº€ã®å…±è‘—ã‚¨ãƒƒã‚¸ã¯éè¡¨ç¤ºã€‚å€¤ã‚’ä¸Šã’ã‚‹ã»ã©â€œã‚ˆãçµ„ã‚€â€å¼·ã„é–¢ä¿‚ã ã‘ãŒæ®‹ã‚Šã¾ã™ã€‚"
            )
        with c7:
            must_sel_labels = st.multiselect(
                "å¿…é ˆï¼ˆè‘—è€…åãƒ»èª­ã¿ã§æ¤œç´¢å¯ï¼‰",
                options=_author_labels,
                default=[],
                key="res_net_must_ms",
            )
            must_sel = [_label_to_name.get(x, x) for x in must_sel_labels]
        with c8:
            excl_sel_labels = st.multiselect(
                "é™¤å¤–ï¼ˆè‘—è€…åãƒ»èª­ã¿ã§æ¤œç´¢å¯ï¼‰",
                options=_author_labels,
                default=[],
                key="res_net_excl_ms",
            )
            excl_sel = [_label_to_name.get(x, x) for x in excl_sel_labels]

        # ã‚¨ãƒƒã‚¸æ§‹ç¯‰ï¼ˆãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å¾“æ¥ã©ãŠã‚Šåˆ©ç”¨å¯ï¼‰
        _tg_key = ",".join(tg_sel) if tg_sel else ""
        _tp_key = ",".join(tp_sel) if tp_sel else ""
        cache_key = f"coauth_edges|{y_from}-{y_to}|tg{_tg_key}|tp{_tp_key}"
        edges = None
        if use_disk_cache and HAS_DISK_CACHE:
            path = cache_csv_path("coauthor_edges", cache_key)
            cached = load_csv_if_exists(path)
            if cached is not None:
                edges = cached
        if edges is None:
            # æ–°ç‰ˆ compute API ã¯å¹´/å¯¾è±¡ç‰©/ã‚¿ã‚¤ãƒ—ã‚’æ¸¡ã™ï¼ˆå¾Œæ–¹äº’æ›ãŒã‚ã‚Œã° try/exceptï¼‰
            try:
                edges = build_coauthor_edges(df_use, y_from, y_to, tg_sel, tp_sel)
            except TypeError:
                edges = build_coauthor_edges(df_use)
            if use_disk_cache and HAS_DISK_CACHE and edges is not None:
                save_csv(edges, cache_csv_path("coauthor_edges", cache_key))

        # --- å¿…é ˆï¼é™¤å¤–ï¼ˆã‚µã‚¸ã‚§ã‚¹ãƒˆé¸æŠï¼‰ãƒ•ã‚£ãƒ«ã‚¿ã‚’ã‚¨ãƒƒã‚¸ã«é©ç”¨ ---
        if edges is not None and not edges.empty:
            if must_sel:
                ms = set(must_sel)
                edges = edges[edges.apply(lambda r: (r["src"] in ms) or (r["dst"] in ms), axis=1)]
            if excl_sel:
                es = set(excl_sel)
                edges = edges[~edges.apply(lambda r: (r["src"] in es) or (r["dst"] in es), axis=1)]
            edges = edges.reset_index(drop=True)

        if edges is None or edges.empty:
            st.info("å…±è‘—é–¢ä¿‚ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¡ä»¶ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        else:
            # --- ãƒãƒ¼ãƒ‰è‰²ãƒãƒƒãƒ—ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æç”»ã¨åŒæœŸç”¨ï¼‰ ---
            node_color_map = None
            st.markdown(
                """
                <div style="display:flex; align-items:center; gap:6px; margin:6px 0 2px 0;">
                  <span style="font-weight:600; font-size:0.95rem; opacity:0.9;">ğŸ” ç ”ç©¶è€…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¦ç´„ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿è‰²é€£å‹•ï¼‰</span>
                </div>
                """,
                unsafe_allow_html=True,
            )
            rank = centrality_from_edges(edges, metric=metric).head(int(top_n))
            st.caption("â€» æŒ‡æ¨™ã®æ„å‘³ï¼šæ¬¡æ•°ï¼ã¤ãªãŒã‚Šã®æ•°ï½œåª’ä»‹ï¼æ©‹æ¸¡ã—åº¦ï½œå›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ï¼å½±éŸ¿åŠ›ï¼ˆæœ‰åŠ›è€…ã¨ã®çµã³ä»˜ãï¼‰")
            st.caption("â€» å¿…é ˆï¼šé¸ã‚“ã è‘—è€…ã‚’å«ã‚€ã‚¨ãƒƒã‚¸ã ã‘è¡¨ç¤ºï¼é™¤å¤–ï¼šå«ã‚€ã‚¨ãƒƒã‚¸ã‚’é™¤å¤–ï¼ˆæ¼¢å­—ãƒ»â€œã‚ˆã¿â€ã§æ¤œç´¢å¯ï¼‰ã€‚")

            # --- ä¸­å¿ƒè‘—è€…ï¼‹è¿‘å‚ã‚µãƒãƒªãƒ¼ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿è‰²ã¨é€£å‹•ï¼‰ ---
            try:
                if HAS_NX:
                    _edges_for_summary = edges[edges["weight"] >= int(min_w)].copy()
                    Gsum = nx.Graph()
                    for _, r in _edges_for_summary.iterrows():
                        Gsum.add_edge(str(r["src"]), str(r["dst"]), weight=float(r["weight"]))
                    if Gsum.number_of_nodes() > 0:
                        try:
                            from networkx.algorithms.community import louvain_communities
                            _comms = list(louvain_communities(Gsum, weight="weight", resolution=1.3))
                        except Exception:
                            from networkx.algorithms.community import greedy_modularity_communities
                            _comms = list(greedy_modularity_communities(Gsum, weight="weight"))
                        _comm_id = {}
                        for i, cset in enumerate(_comms):
                            for n in cset:
                                _comm_id[n] = i

                        _central_nodes = rank["è‘—è€…"].tolist()
                        _rows = []
                        for author in _central_nodes:
                            if author not in Gsum:
                                continue
                            partners = []
                            for nbr in Gsum.neighbors(author):
                                w = float(Gsum[author][nbr].get("weight", 1.0))
                                partners.append((nbr, w))
                            partners.sort(key=lambda x: (-x[1], x[0]))
                            uniq_partners = [p for p, _ in partners]
                            top_partners = uniq_partners[:3]

                            titles = []
                            _title_cols = ["ã‚¿ã‚¤ãƒˆãƒ«", "è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«", "title", "Title", "é¡Œå"]
                            for _, rr in df_use.iterrows():
                                names = list(dict.fromkeys(split_authors(rr.get("è‘—è€…", ""))))
                                if (author in names) and any(tp in names for tp in top_partners):
                                    t = ""
                                    for _c in _title_cols:
                                        if _c in rr and pd.notna(rr[_c]) and str(rr[_c]).strip():
                                            t = str(rr[_c]).strip()
                                            break
                                    if t:
                                        titles.append(t)
                                if len(titles) >= 3:
                                    break

                            cid = int(_comm_id.get(author, 0))
                            ccolor = _PALETTE[cid % len(_PALETTE)]

                            _rows.append({
                                "cluster_id": cid,
                                "cluster_color": ccolor,
                                "cluster": " ",
                                "ä¸­å¿ƒè‘—è€…": author,
                                "ç›¸æ‰‹è‘—è€…æ•°": len(set(uniq_partners)),
                                "ä»£è¡¨ç›¸æ‰‹ï¼ˆä¸Šä½3åï¼‰": "ï¼".join(top_partners),
                                "example_titles": "ï¼".join(titles)
                            })

                        if _rows:
                            _sum_df = pd.DataFrame(
                                _rows,
                                columns=[
                                    "cluster",
                                    "ä¸­å¿ƒè‘—è€…",
                                    "ç›¸æ‰‹è‘—è€…æ•°",
                                    "ä»£è¡¨ç›¸æ‰‹ï¼ˆä¸Šä½3åï¼‰",
                                    "example_titles",
                                    "cluster_id",
                                    "cluster_color",
                                ],
                            )
                            _rank_for_merge = rank[["è‘—è€…", "å…±è‘—æ•°"]].rename(columns={"è‘—è€…": "ä¸­å¿ƒè‘—è€…"})
                            _merged = pd.merge(_sum_df, _rank_for_merge, on="ä¸­å¿ƒè‘—è€…", how="left")

                            _disp = _merged.rename(columns={"ä¸­å¿ƒè‘—è€…": "è‘—è€…", "example_titles": "è«–æ–‡ä¾‹"}).copy()
                            node_color_map = {str(a): str(c) for a, c in _merged[["ä¸­å¿ƒè‘—è€…", "cluster_color"]].dropna().values}
                            _disp = _disp[["cluster", "è‘—è€…", "å…±è‘—æ•°", "ç›¸æ‰‹è‘—è€…æ•°", "ä»£è¡¨ç›¸æ‰‹ï¼ˆä¸Šä½3åï¼‰", "è«–æ–‡ä¾‹"]]
                            _disp["å…±è‘—æ•°"] = _disp["å…±è‘—æ•°"].fillna(0).astype(int)
                            _disp["ç›¸æ‰‹è‘—è€…æ•°"] = _disp["ç›¸æ‰‹è‘—è€…æ•°"].fillna(0).astype(int)

                            _disp = _merged.rename(columns={"ä¸­å¿ƒè‘—è€…": "è‘—è€…", "example_titles": "è«–æ–‡ä¾‹"}).copy()
                            _disp["cluster_img"] = _merged["cluster_color"].map(lambda c: _color_square_data_uri(c))
                            _disp = _disp[["cluster_img", "è‘—è€…", "å…±è‘—æ•°", "ç›¸æ‰‹è‘—è€…æ•°", "ä»£è¡¨ç›¸æ‰‹ï¼ˆä¸Šä½3åï¼‰", "è«–æ–‡ä¾‹"]]
                            _disp["å…±è‘—æ•°"] = _disp["å…±è‘—æ•°"].fillna(0).astype(int)
                            _disp["ç›¸æ‰‹è‘—è€…æ•°"] = _disp["ç›¸æ‰‹è‘—è€…æ•°"].fillna(0).astype(int)
                            _disp = _disp.rename(columns={"cluster_img": "cluster"})

                            st.markdown(
                                "<div style='display:flex; align-items:center; gap:6px; margin:10px 0 4px 0;'>"
                                "<span style='font-weight:600; font-size:0.95rem; opacity:0.9;'>ğŸ§­ ä¸­å¿ƒè‘—è€…ã‚µãƒãƒªãƒ¼ï¼ˆclusterè‰²é€£å‹•ï¼‰</span>"
                                "</div>",
                                unsafe_allow_html=True,
                            )
                            st.markdown("<style>.stDataFrame [data-testid='stImage'] img { display:block; margin:auto; }</style>", unsafe_allow_html=True)
                            st.dataframe(
                                _disp,
                                column_config={
                                    "cluster": st.column_config.ImageColumn(
                                        "cluster",
                                        help="ã‚¯ãƒ©ã‚¹ã‚¿è‰²ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨åŒæœŸï¼‰",
                                        width="small",
                                    ),
                                },
                                use_container_width=True,
                                hide_index=True,
                            )

                            _size_by_cluster = (
                                _merged.groupby("cluster_id").size().reset_index(name="size").sort_values("size", ascending=False)
                            )
                            _cid_to_rank = {int(cid): i + 1 for i, cid in enumerate(_size_by_cluster["cluster_id"].tolist())}

                            _legend_parts = [
                                "<div style='display:flex; align-items:center; gap:10px; flex-wrap:wrap; margin:6px 0 2px 0;'>",
                                "<span style='font-weight:700;'>ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å‡¡ä¾‹</span>",
                            ]
                            for _, rr in _size_by_cluster.merge(
                                _merged[["cluster_id", "cluster_color"]].drop_duplicates(), on="cluster_id", how="left"
                            ).iterrows():
                                rank_no = _cid_to_rank.get(int(rr["cluster_id"]), 0)
                                color = rr["cluster_color"]
                                count = int(rr["size"])
                                _legend_parts.append(
                                    f"<span style='display:inline-flex; align-items:center; gap:6px;'>"
                                    f"<span style='display:inline-block; width:12px; height:12px; background:{color}; border-radius:2px;'></span>"
                                    f"<span style='font-size:13px; opacity:0.9;'>C{rank_no}ï¼ˆ{count}åï¼‰</span>"
                                    f"</span>"
                                )
                            _legend_parts.append("</div>")
                            st.markdown("".join(_legend_parts), unsafe_allow_html=True)

                            # æ¡ä»¶ã‚µãƒãƒªãƒ¼
                            st.caption(
                                "æ¡ä»¶ï¼š"
                                f"{_METRIC_JA.get(str(metric), str(metric))} ï½œ "
                                f"ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä»¶æ•°ï¼š{int(top_n)} ï½œ æœ€å°å…±è‘—å›æ•°ï¼š{int(min_w)} ï½œ "
                                f"å¿…é ˆï¼š{len(must_sel)}ä»¶ï¼é™¤å¤–ï¼š{len(excl_sel)}ä»¶ ï½œ "
                                + banners.summarize(GlobalFilters(y_from, y_to, tg_sel, tp_sel))
                            )
            except Exception as _e:
                st.caption(f"ä¸­å¿ƒè‘—è€…ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {_e!s}")
                st.caption(
                    "æ¡ä»¶ï¼š"
                    f"{_METRIC_JA.get(str(metric), str(metric))} ï½œ "
                    f"ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä»¶æ•°ï¼š{int(top_n)} ï½œ æœ€å°å…±è‘—å›æ•°ï¼š{int(min_w)} ï½œ "
                    f"å¿…é ˆï¼š{len(must_sel)}ä»¶ï¼é™¤å¤–ï¼š{len(excl_sel)}ä»¶ ï½œ "
                    + banners.summarize(GlobalFilters(y_from, y_to, tg_sel, tp_sel))
                )

            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ï¼ˆä¸Šä½ï¼‰
            # st.dataframe(rank, use_container_width=True, hide_index=True)
            copyui.expander("ğŸ“‹ è‘—è€…åã‚’ã™ãã‚³ãƒ”ãƒ¼", rank["è‘—è€…"].tolist())
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æç”»ï¼ˆé…å»¶ï¼‰
            with st.expander("ğŸ•¸ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å¯è¦–åŒ–", expanded=False):
                vc1, vc2 = st.columns([1,1])
                with vc1:
                    top_only_cb = st.checkbox(
                        "ä¸Šä½ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®å‘¨è¾ºã ã‘è¡¨ç¤º",
                        value=True,
                        key="res_net_toponly_cb",
                        help="ä¸Šä½ã«é¸ã°ã‚ŒãŸè‘—è€…æœ¬äººã¨ã€ãã®ç›´æ¥ã®å…±è‘—è€…ã ã‘ã§ã‚µãƒ–ã‚°ãƒ©ãƒ•ã‚’ä½œã‚Šã¾ã™ã€‚"
                    )
                with vc2:
                    fixed_layout = st.checkbox(
                        "ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’å›ºå®š",
                        value=False,
                        key="res_net_fixed",
                        help="ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ­¢ã‚ã€é…ç½®ã‚’å›ºå®šã—ã¾ã™ï¼ˆä½ç½®ãŒã¶ã‚Œã¾ã›ã‚“ï¼‰ã€‚"
                    )
                if st.button("ğŸŒ æç”»ã™ã‚‹", key="res_net_draw"):
                    top_nodes = rank["è‘—è€…"].tolist() if top_only_cb else None
                    draw_network(
                        edges,
                        top_nodes=top_nodes,
                        min_weight=int(min_w),
                        height_px=700,
                        physics_enabled=(not fixed_layout),
                        node_color_map=node_color_map
                    )

    # ===== â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ =====
    with tab_trend:
        yearly = yearly_author_counts(df_use)
        if yearly.empty:
            st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        tot = yearly.groupby("è‘—è€…")["count"].sum().sort_values(ascending=False)
        options = tot.index.tolist()
        col_a, col_b, col_c = st.columns([1, 7, 1])
        with col_a:
            max_auth = st.number_input("åˆæœŸè¡¨ç¤ºæ•°ï¼ˆä¸Šä½ï¼‰", min_value=3, max_value=30, value=10, step=1, key="res_trend_initn")
        default_sel = options[: int(max_auth)]
        with col_b:
            sel = st.multiselect("è¡¨ç¤ºã™ã‚‹è‘—è€…ï¼ˆè¤‡æ•°å¯ï¼‰", options, default=default_sel, key="res_trend_authors")
        with col_c:
            ma = st.number_input("ç§»å‹•å¹³å‡ï¼ˆå¹´ï¼‰", min_value=1, max_value=7, value=1, step=1, key="res_trend_ma")

        piv = yearly.pivot_table(index="ç™ºè¡Œå¹´", columns="è‘—è€…", values="count", aggfunc="sum").fillna(0).sort_index()
        if sel:
            piv = piv[[c for c in sel if c in piv.columns]]
        if piv.shape[1] == 0:
            st.info("è¡¨ç¤ºå¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        if int(ma) > 1:
            piv = piv.rolling(window=int(ma), min_periods=1).mean()

        metric_mode = st.radio("è¡¨ç¤ºæŒ‡æ¨™", ["ä»¶æ•°", "ã‚·ã‚§ã‚¢(%)"], horizontal=True, key="res_trend_metric")
        if metric_mode == "ã‚·ã‚§ã‚¢(%)":
            row_sums = piv.sum(axis=1)
            piv = piv.div(row_sums, axis=0).fillna(0) * 100

        if not piv.empty:
            try:
                last_row = piv.iloc[-1]
            except Exception:
                last_row = piv.mean(axis=0, numeric_only=True)
            order = list(last_row.sort_values(ascending=False).index)
            piv = piv.loc[:, [c for c in order if c in piv.columns]]

        try:
            import plotly.express as px
            fig = px.line(piv.reset_index().melt(id_vars="ç™ºè¡Œå¹´", var_name="è‘—è€…", value_name="å€¤"),
                          x="ç™ºè¡Œå¹´", y="å€¤", color="è‘—è€…", markers=True)
            fig.update_layout(height=520, margin=dict(l=10,r=10,t=30,b=10), legend_title_text="è‘—è€…",
                              yaxis_title=("ä»¶æ•°" if metric_mode=="ä»¶æ•°" else "ã‚·ã‚§ã‚¢(%)"))
            st.plotly_chart(fig, use_container_width=True)
        except Exception:
            st.line_chart(piv)

        st.caption(f"æ¡ä»¶ï¼šè¡¨ç¤ºè‘—è€…={len(sel)}å ï½œ ç§»å‹•å¹³å‡={int(ma)}å¹´ ï½œ æŒ‡æ¨™={metric_mode} ï½œ " + _summarize(y_from, y_to, tg_sel, tp_sel))


        # --- æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ã«å¯¾å¿œã—ãŸè¡¨ï¼ˆæŠ˜ã‚Šç•³ã¿å¼ï¼‰ã‚’è¡¨ç¤º ---
        with st.expander("ğŸ“Š è¡¨ã‚’è¡¨ç¤ºï¼ˆæŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ã«å¯¾å¿œï¼‰", expanded=False):
            try:
                # piv ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ç™ºè¡Œå¹´ã«ãªã£ã¦ã„ã‚‹æƒ³å®šãªã®ã§åˆ—ã«æˆ»ã™
                tbl = piv.copy()
                tbl_display = tbl.reset_index()

                # --- ç™ºè¡Œå¹´ã®æ­£è¦åŒ–: ã‚«ãƒ³ãƒé™¤å»ã¨æ•´æ•°åŒ–ï¼ˆä¾‹: '1,988' -> 1988ï¼‰ ---
                if "ç™ºè¡Œå¹´" in tbl_display.columns:
                    def _fmt_year_str(v):
                        # Return a string representation without commas. Prefer integer form when possible.
                        try:
                            if pd.isna(v):
                                return ""
                            # if already numeric-like
                            num = float(v)
                            return str(int(num))
                        except Exception:
                            s = str(v).replace(",", "").strip()
                            if s in ("", "nan"):
                                return ""
                            try:
                                return str(int(float(s)))
                            except Exception:
                                return s

                    tbl_display["ç™ºè¡Œå¹´"] = tbl_display["ç™ºè¡Œå¹´"].apply(_fmt_year_str)

                # è¡¨ç¤ºï¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                st.dataframe(tbl_display, use_container_width=True, hide_index=False)
                st.download_button(
                    "ğŸ“¥ è¡¨ã‚’CSVã§ä¿å­˜",
                    data=tbl_display.to_csv(index=False).encode("utf-8"),
                    file_name="coauthor_trend_table.csv",
                    mime="text/csv",
                    key="dl_coauthor_trend_table",
                )
            except Exception as _e:
                st.caption(f"è¡¨ã®è¡¨ç¤ºã«å¤±æ•—ã—ã¾ã—ãŸ: {_e!s}")

        copyui.expander("ğŸ“‹ è‘—è€…åã‚’ã™ãã‚³ãƒ”ãƒ¼", list(piv.columns))

    # â‘¥ å¯¾è±¡ç‰©åˆ¥ã®Top5è‘—è€…ï¼ˆæ”¹å–„ç‰ˆUIï¼‰: move inside tab_count, after caption and before copyui.expander
    with tab_count:
        # ... (existing code above remains unchanged)
        if s.empty:
            st.info("æ¡ä»¶ã«åˆã†ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            rank = s.reset_index()
            rank.columns = ["è‘—è€…", "è«–æ–‡æ•°"]
            rank = rank.sort_values(["è«–æ–‡æ•°", "è‘—è€…"], ascending=[False, True])
            rank_shown = rank.head(int(top_n))


            # â‘¥ å¯¾è±¡ç‰©åˆ¥ã®Top5è‘—è€…ï¼ˆæ”¹å–„ç‰ˆUIï¼‰: only in è«–æ–‡æ•°ã‚µãƒ–ã‚¿ãƒ–
            with st.expander("ğŸ·ï¸ å¯¾è±¡ç‰©åˆ¥ã®Top5è‘—è€…ï¼ˆç¾åœ¨ã®ãƒ•ã‚£ãƒ«ã‚¿ã§é›†è¨ˆï¼‰", expanded=False):
                # â–¼ è¦‹ã‚„ã™ã•æ”¹å–„ç‰ˆï¼šå¯¾è±¡ç‰©ã”ã¨ã®Top5ã‚’ã€Œæ¨ªæ£’ã‚°ãƒ©ãƒ•ã®å°ã‚«ãƒ¼ãƒ‰ã€ã§ä¸¦ã¹ã‚‹ï¼ˆæœ€å¤§8ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰
                view_mode = st.radio("è¡¨ç¤ºå½¢å¼", ["ã‚°ãƒ©ãƒ•", "è¡¨"], horizontal=True, key="res_cnt_tg_view")
                try:
                    # å¯¾è±¡ç‰©ã”ã¨ã«è‘—è€…ã‚«ã‚¦ãƒ³ãƒˆ
                    rows = []
                    for _, r in df_rank.iterrows():
                        import re
                        split_multi = lambda s: [w.strip() for w in re.split(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\\sã€€]+", str(s or "")) if w.strip()]
                        tg_list = list(dict.fromkeys(split_multi(r.get("å¯¾è±¡ç‰©_top3", ""))))
                        names = list(dict.fromkeys(split_authors(r.get("è‘—è€…", ""))))
                        for tg in tg_list:
                            for n in names:
                                if tg and n:
                                    rows.append((tg, n))
                    if not rows:
                        st.caption("å¯¾è±¡ç‰©åˆ¥ã®ä¸Šä½æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                    else:
                        df_tg = pd.DataFrame(rows, columns=["å¯¾è±¡ç‰©", "è‘—è€…"]).value_counts().reset_index(name="ä»¶æ•°")
                        # å¤šã™ãã‚‹å¯¾è±¡ç‰©ã¯ä¸Šä½ã®ã‚‚ã®ã ã‘è¡¨ç¤ºï¼ˆæœ€å¤§8ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰
                        heads = df_tg.groupby("å¯¾è±¡ç‰©")["ä»¶æ•°"].sum().sort_values(ascending=False).head(8).index.tolist()
                        show = (
                            df_tg[df_tg["å¯¾è±¡ç‰©"].isin(heads)]
                            .sort_values(["å¯¾è±¡ç‰©", "ä»¶æ•°"], ascending=[True, False])
                            .groupby("å¯¾è±¡ç‰©")
                            .head(5)
                            .reset_index(drop=True)
                        )

                        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆCSVï¼‰
                        st.download_button(
                            "ğŸ“¥ ã“ã®ä¸€è¦§ã‚’CSVã§ä¿å­˜",
                            data=show.to_csv(index=False).encode("utf-8"),
                            file_name="target_top5_authors.csv",
                            mime="text/csv",
                            key="dl_target_top5_authors"
                        )

                        if view_mode == "è¡¨":
                            st.dataframe(show, use_container_width=True, hide_index=True)
                        else:
                            try:
                                import plotly.express as px
                                # å¯¾è±¡ç‰©ã”ã¨ã«2åˆ—ã®ã‚«ãƒ¼ãƒ‰é…ç½®ã§å¯èª­æ€§UP
                                cols = st.columns(2)
                                for i, tg in enumerate(heads):
                                    sub = show[show["å¯¾è±¡ç‰©"] == tg].copy()
                                    # æ¨ªæ£’ç”¨ã«ä¸¦ã¹æ›¿ãˆï¼ˆå°ã•ã„â†’å¤§ãã„ã§ç©ã¿ä¸ŠãŒã‚‹è¦–è¦šã‚’ä½œã‚‹ï¼‰
                                    sub = sub.sort_values("ä»¶æ•°", ascending=True)
                                    with cols[i % 2]:
                                        fig = px.bar(
                                            sub,
                                            x="ä»¶æ•°",
                                            y="è‘—è€…",
                                            orientation="h",
                                            text_auto=True,
                                            title=tg
                                        )
                                        fig.update_layout(
                                            height=260,
                                            margin=dict(l=8, r=8, t=36, b=8),
                                            xaxis_title=None,
                                            yaxis_title=None
                                        )
                                        st.plotly_chart(fig, use_container_width=True)
                            except Exception:
                                # PlotlyãŒç„¡ã„å ´åˆã¯å¯¾è±¡ç‰©ã”ã¨ã«å°ã•ãªè¡¨ã§ä»£æ›¿
                                cols = st.columns(2)
                                for i, tg in enumerate(heads):
                                    sub = show[show["å¯¾è±¡ç‰©"] == tg].sort_values("ä»¶æ•°", ascending=False)
                                    with cols[i % 2]:
                                        st.markdown(f"**{tg}**")
                                        st.dataframe(sub[["è‘—è€…", "ä»¶æ•°"]], use_container_width=True, hide_index=True)
                except Exception as e:
                    st.caption(f"å¯¾è±¡ç‰©åˆ¥Topã®é›†è¨ˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e!s}")

            copyui.expander("ğŸ“‹ è‘—è€…åã‚’ã™ãã‚³ãƒ”ãƒ¼", rank_shown["è‘—è€…"].tolist(), height=140)