# -*- coding: utf-8 -*-
"""
è«–æ–‡æ¤œç´¢UIï¼ˆãƒ•ã‚©ãƒ¼ãƒ ä¸€æ‹¬åæ˜ ç‰ˆï¼‰
- å·¦ã®ãƒ•ã‚£ãƒ«ã‚¿ã§çµžã‚Šè¾¼ã¿
- ä¸Šï¼šæ¤œç´¢çµæžœãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆâ˜…ãƒã‚§ãƒƒã‚¯å¯ï¼ãƒ•ã‚©ãƒ¼ãƒ å†…ã§ä¸€æ‹¬åæ˜ ï¼‰
- ä¸‹ï¼šãŠæ°—ã«å…¥ã‚Šä¸€è¦§ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ç„¡è¦–ã§å…¨ä½“ã‹ã‚‰è¡¨ç¤ºï¼â˜…ãƒã‚§ãƒƒã‚¯å¯ï¼ãƒ•ã‚©ãƒ¼ãƒ å†…ã§ä¸€æ‹¬åæ˜ ï¼‰
- HP/PDF ã¯ãƒªãƒ³ã‚¯åŒ–
- ã€Œâ˜…ã€æ“ä½œã«ã‚ˆã‚‹ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚¸ãƒ£ãƒ³ãƒ—ã‚’é˜²ããŸã‚ã€st.form ã§ä¸€æ‹¬åæ˜ 
"""
import io, re, time
import pandas as pd
import requests
import streamlit as st

st.set_page_config(page_title="è«–æ–‡æ¤œç´¢ï¼ˆçµ±ä¸€UIç‰ˆï¼‰", layout="wide")

# ===== åˆ—å®šç¾© =====
KEY_COLS = [
    "llm_keywords","primary_keywords","secondary_keywords","featured_keywords",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰4","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰5",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰6","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰7","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰8","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰9","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰10",
]
TARGET_ORDER = [
    "æ¸…é…’","ãƒ“ãƒ¼ãƒ«","ãƒ¯ã‚¤ãƒ³","ç„¼é…Ž","ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«é£²æ–™","ç™ºé…µä¹³ãƒ»ä¹³è£½å“",
    "é†¤æ²¹","å‘³å™Œ","ç™ºé…µé£Ÿå“","è¾²ç”£ç‰©ãƒ»æžœå®Ÿ","å‰¯ç”£ç‰©ãƒ»ãƒã‚¤ã‚ªãƒžã‚¹","é…µæ¯ãƒ»å¾®ç”Ÿç‰©","ãã®ä»–"
]
TYPE_ORDER = [
    "å¾®ç”Ÿç‰©ãƒ»éºä¼å­é–¢é€£","é†¸é€ å·¥ç¨‹ãƒ»è£½é€ æŠ€è¡“","å¿œç”¨åˆ©ç”¨ãƒ»é£Ÿå“é–‹ç™º","æˆåˆ†åˆ†æžãƒ»ç‰©æ€§è©•ä¾¡",
    "å“è³ªè©•ä¾¡ãƒ»å®˜èƒ½è©•ä¾¡","æ­´å²ãƒ»æ–‡åŒ–ãƒ»çµŒæ¸ˆ","å¥åº·æ©Ÿèƒ½ãƒ»æ „é¤ŠåŠ¹æžœ","çµ±è¨ˆè§£æžãƒ»ãƒ¢ãƒ‡ãƒ«åŒ–",
    "ç’°å¢ƒãƒ»ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£","ä¿å­˜ãƒ»å®‰å®šæ€§","ãã®ä»–ï¼ˆç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼‰"
]

# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
def norm_space(s: str) -> str:
    s = str(s or "")
    s = s.replace("\u00A0", " ")
    return re.sub(r"\s+", " ", s).strip()

def norm_key(s: str) -> str:
    return norm_space(s).lower()

AUTHOR_SPLIT_RE = re.compile(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ]+")
def split_authors(cell):
    if not cell: return []
    return [w.strip() for w in AUTHOR_SPLIT_RE.split(str(cell)) if w.strip()]

def split_multi(s):
    if not s: return []
    return [w.strip() for w in re.split(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+", str(s)) if w.strip()]

def tokens_from_query(q):
    q = norm_key(q)
    return [t for t in re.split(r"[ ,ï¼Œã€ï¼›;ã€€]+", q) if t]

def fetch_csv(url: str) -> pd.DataFrame:
    r = requests.get(url, timeout=30)
    r.raise_for_status()
    return pd.read_csv(io.BytesIO(r.content), encoding="utf-8")

def ensure_cols(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    return df

def consolidate_authors_column(df: pd.DataFrame) -> pd.DataFrame:
    """è‘—è€…åˆ—ï¼šç©ºç™½ã§ã¯åˆ†å‰²ã—ãªã„ã€‚åŒºåˆ‡ã‚Šè¨˜å·ã®ã¿ã§åˆ†å‰²â†’ã‚»ãƒ«å†…é‡è¤‡ã‚’ä»£è¡¨è¡¨è¨˜ã«çµ±åˆ"""
    if "è‘—è€…" not in df.columns:
        return df
    df = df.copy()
    def unify(cell: str) -> str:
        names = split_authors(cell)
        seen = set()
        result = []
        for n in names:
            k = norm_key(n)
            if not k or k in seen:
                continue
            seen.add(k)
            result.append(n)  # å…ˆã«å‡ºãŸè¡¨è¨˜ã‚’ä»£è¡¨
        return ", ".join(result)
    df["è‘—è€…"] = df["è‘—è€…"].astype(str).apply(unify)
    return df

def build_author_candidates(df: pd.DataFrame):
    rep = {}
    for v in df.get("è‘—è€…", pd.Series(dtype=str)).fillna(""):
        for name in split_authors(v):
            k = norm_key(name)
            if k and k not in rep:
                rep[k] = name
    return [rep[k] for k in sorted(rep.keys())]

def haystack(row, include_fulltext: bool):
    parts = [
        str(row.get("è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«","")),
        str(row.get("è‘—è€…","")),
        str(row.get("file_name","")),
        " ".join(str(row.get(c,"")) for c in KEY_COLS if c in row),
    ]
    if include_fulltext and "pdf_text" in row:
        parts.append(str(row.get("pdf_text","")))
    return norm_key(" \n ".join(parts))

def to_int_or_none(x):
    try:
        return int(str(x).strip())
    except Exception:
        m = re.search(r"\d+", str(x))
        return int(m.group()) if m else None

def order_by_template(values, template):
    vs = list(dict.fromkeys(values))  # unique & keep order
    tmpl_set = set(template)
    head = [v for v in template if v in vs and "ãã®ä»–" not in v]
    mid  = sorted([v for v in vs if v not in tmpl_set and "ãã®ä»–" not in v])
    tail = [v for v in template if v in vs and "ãã®ä»–" in v] + \
           [v for v in vs if ("ãã®ä»–" in v and v not in template)]
    return head + mid + tail

def make_row_id(row):
    no = str(row.get("No.", "")).strip()
    if no and no.lower() not in {"none", "nan"}:
        return f"NO:{no}"
    ttl = str(row.get("è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«", "")).strip()
    yr  = str(row.get("ç™ºè¡Œå¹´", "")).strip()
    return f"T:{ttl}|Y:{yr}"

# ===== ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ =====
st.title("è«–æ–‡æ¤œç´¢ï¼ˆå¹´ãƒ»å·»ãƒ»å·ï¼‹çµ±ä¸€æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ï¼‰")

with st.sidebar:
    st.header("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    url = st.text_input("å…¬é–‹CSVã®URLï¼ˆGoogleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ output=csvï¼‰", value="")
    up  = st.file_uploader("CSVã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿", type=["csv"])
    if st.button("èª­ã¿è¾¼ã¿", type="primary"):
        try:
            if up is not None:
                st.session_state.df = ensure_cols(pd.read_csv(up))
            elif url.strip():
                st.session_state.df = ensure_cols(fetch_csv(url.strip()))
            else:
                st.warning("URL ã¾ãŸã¯ CSV ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        except Exception as e:
            st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

df = st.session_state.get("df", pd.DataFrame())
if df.empty:
    st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ CSV ã‚’æŒ‡å®šã—ã¦ [èª­ã¿è¾¼ã¿] ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# No. ãŒç©ºã®è¡Œã¯éžè¡¨ç¤º
if "No." in df.columns:
    df = df[df["No."].apply(lambda v: str(v).strip() not in ("", "None", "nan"))]

# è‘—è€…è¡¨è¨˜ã®çµ±åˆ
df = consolidate_authors_column(df)

# ===== å¹´ãƒ»å·»ãƒ»å·ï¼ˆ1è¡Œï¼‰ =====
st.subheader("å¹´ãƒ»å·»ãƒ»å·ãƒ•ã‚£ãƒ«ã‚¿")

year_vals = pd.to_numeric(df.get("ç™ºè¡Œå¹´", pd.Series(dtype=str)), errors="coerce")
if year_vals.notna().any():
    ymin_all, ymax_all = int(year_vals.min()), int(year_vals.max())
else:
    ymin_all, ymax_all = 1980, 2025

c_y, c_v, c_i = st.columns([1, 1, 1])
with c_y:
    y_from, y_to = st.slider(
        "ç™ºè¡Œå¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin_all, max_value=ymax_all,
        value=(ymin_all, ymax_all)  # å…¨ç¯„å›²ã‚’åˆæœŸå€¤ã«
    )
with c_v:
    vol_candidates = sorted({v for v in (df.get("å·»æ•°", pd.Series(dtype=str)).map(to_int_or_none)).dropna().unique()})
    vols_sel = st.multiselect("å·»ï¼ˆæ•´æ•°ãƒ»è¤‡æ•°é¸æŠžï¼‰", vol_candidates, default=[])
with c_i:
    iss_candidates = sorted({v for v in (df.get("å·æ•°", pd.Series(dtype=str)).map(to_int_or_none)).dropna().unique()})
    issues_sel = st.multiselect("å·ï¼ˆæ•´æ•°ãƒ»è¤‡æ•°é¸æŠžï¼‰", iss_candidates, default=[])

# ===== è‘—è€…ãƒ»å¯¾è±¡ç‰©ãƒ»ç ”ç©¶ã‚¿ã‚¤ãƒ—ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ =====
st.subheader("çµ±ä¸€æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿")

c_a, c_tg, c_tp = st.columns([1.2, 1.2, 1.2])
with c_a:
    authors_all = build_author_candidates(df)
    authors_sel = st.multiselect("è‘—è€…ï¼ˆæ­£è¦åŒ–ï¼‹å€‹åˆ¥ï¼‰", authors_all, default=[])

with c_tg:
    raw_targets = {t for v in df.get("å¯¾è±¡ç‰©", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
    targets_all = order_by_template(list(raw_targets), TARGET_ORDER)
    targets_sel = st.multiselect("å¯¾è±¡ç‰©ï¼ˆè¤‡æ•°é¸æŠžï¼éƒ¨åˆ†ä¸€è‡´ï¼‰", targets_all, default=[])

with c_tp:
    raw_types = {t for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
    types_all = order_by_template(list(raw_types), TYPE_ORDER)
    types_sel = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼ˆè¤‡æ•°é¸æŠžï¼éƒ¨åˆ†ä¸€è‡´ï¼‰", types_all, default=[])

c_kw1, c_kw2, c_kw3 = st.columns([3, 1, 1])
with c_kw1:
    kw_query = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç©ºç™½/ã‚«ãƒ³ãƒžã§è¤‡æ•°å¯ï¼‰", value="")
with c_kw2:
    kw_mode = st.radio("ä¸€è‡´æ¡ä»¶", ["OR", "AND"], index=0, horizontal=True)
with c_kw3:
    include_fulltext = st.checkbox("æœ¬æ–‡ã‚‚æ¤œç´¢ï¼ˆpdf_textï¼‰", value=True)

# ===== ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ =====
def apply_filters(_df: pd.DataFrame) -> pd.DataFrame:
    df2 = _df.copy()

    # å¹´
    if "ç™ºè¡Œå¹´" in df2.columns:
        y = pd.to_numeric(df2["ç™ºè¡Œå¹´"], errors="coerce")
        df2 = df2[(y >= y_from) & (y <= y_to) | y.isna()]

    # å·»ãƒ»å·
    if vols_sel and "å·»æ•°" in df2.columns:
        df2 = df2[df2["å·»æ•°"].map(to_int_or_none).isin(set(vols_sel))]
    if issues_sel and "å·æ•°" in df2.columns:
        df2 = df2[df2["å·æ•°"].map(to_int_or_none).isin(set(issues_sel))]

    # è‘—è€…ï¼ˆç©ºç™½ã§åˆ†å‰²ã—ãªã„ï¼‰
    if authors_sel and "è‘—è€…" in df2.columns:
        sel = {norm_key(a) for a in authors_sel}
        def hit_author(v):
            return any(norm_key(x) in sel for x in split_authors(v))
        df2 = df2[df2["è‘—è€…"].apply(hit_author)]

    # å¯¾è±¡ç‰© / ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼šORï¼‰
    if targets_sel and "å¯¾è±¡ç‰©" in df2.columns:
        t_norm = [norm_key(t) for t in targets_sel]
        df2 = df2[df2["å¯¾è±¡ç‰©"].apply(lambda v: any(t in norm_key(v) for t in t_norm))]
    if types_sel and "ç ”ç©¶ã‚¿ã‚¤ãƒ—" in df2.columns:
        t_norm = [norm_key(t) for t in types_sel]
        df2 = df2[df2["ç ”ç©¶ã‚¿ã‚¤ãƒ—"].apply(lambda v: any(t in norm_key(v) for t in t_norm))]

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    toks = tokens_from_query(kw_query)
    if toks:
        def hit_kw(row):
            hs = haystack(row, include_fulltext=include_fulltext)
            return all(t in hs for t in toks) if kw_mode == "AND" else any(t in hs for t in toks)
        df2 = df2[df2.apply(hit_kw, axis=1)]
    return df2

filtered = apply_filters(df)

# ===== è¡¨ã®è¦‹ãŸç›®åˆ¶å¾¡ï¼ˆéš ã—åˆ—ãƒ»ãƒªãƒ³ã‚¯åŒ–ãªã©ï¼‰ =====
st.markdown("### æ¤œç´¢çµæžœ")
st.caption(f"{len(filtered)} / {len(df)} ä»¶")

# éžè¡¨ç¤ºåˆ—ï¼šç›¸å¯¾PASSã€çµ‚äº†ãƒšãƒ¼ã‚¸ã€file_pathã€num_pagesã€file_nameã€llm_keywordsä»¥é™ã™ã¹ã¦
all_cols = list(filtered.columns)
hide_cols = {"ç›¸å¯¾PASS", "çµ‚äº†ãƒšãƒ¼ã‚¸", "file_path", "num_pages", "file_name"}
if "llm_keywords" in all_cols:
    start = all_cols.index("llm_keywords")
    hide_cols.update(all_cols[start:])  # llm_keywords ä»¥é™ã‚’éžè¡¨ç¤º
visible_cols = [c for c in all_cols if c not in hide_cols]

# å³ï¼ˆãŠæ°—ã«å…¥ã‚Šï¼‰å´ã¯ãƒ•ã‚£ãƒ«ã‚¿ç„¡è¦–ã®å…¨ä½“å¯è¦–åˆ—
all_cols_full = list(df.columns)
hide_cols_full = {"ç›¸å¯¾PASS", "çµ‚äº†ãƒšãƒ¼ã‚¸", "file_path", "num_pages", "file_name"}
if "llm_keywords" in all_cols_full:
    start_full = all_cols_full.index("llm_keywords")
    hide_cols_full.update(all_cols_full[start_full:])
visible_cols_full = [c for c in all_cols_full if c not in hide_cols_full]

# å¯è¦–ãƒ‡ãƒ¼ã‚¿
disp = filtered[visible_cols].copy()

# ä¸€æ„IDï¼ˆãŠæ°—ã«å…¥ã‚Šç®¡ç†ç”¨ï¼‰
disp["_row_id"] = disp.apply(make_row_id, axis=1)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ãŠæ°—ã«å…¥ã‚Šé›†åˆ
if "favs" not in st.session_state:
    st.session_state.favs = set()

# ç¾åœ¨ã®ãŠæ°—ã«å…¥ã‚Šåæ˜ ï¼ˆâ˜…åˆæœŸå€¤ï¼‰
disp["â˜…"] = disp["_row_id"].apply(lambda rid: rid in st.session_state.favs)

# LinkColumn è¨­å®š
column_config_main = {
    "â˜…": st.column_config.CheckboxColumn("â˜…", help="æ°—ã«ãªã‚‹è«–æ–‡ã«ãƒã‚§ãƒƒã‚¯/è§£é™¤", default=False, width="small"),
}
if "HPãƒªãƒ³ã‚¯å…ˆ" in disp.columns:
    column_config_main["HPãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("HPãƒªãƒ³ã‚¯å…ˆ", help="å¤–éƒ¨ã‚µã‚¤ãƒˆã¸ç§»å‹•", display_text="HP")
if "PDFãƒªãƒ³ã‚¯å…ˆ" in disp.columns:
    column_config_main["PDFãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("PDFãƒªãƒ³ã‚¯å…ˆ", help="PDFã‚’é–‹ã", display_text="PDF")

display_order = ["â˜…"] + [c for c in disp.columns if c not in ["â˜…", "_row_id"]] + ["_row_id"]

# ===== ä¸Šï¼šãƒ¡ã‚¤ãƒ³è¡¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒ ä¸€æ‹¬åæ˜ ï¼‰ =====
st.subheader("å…¨ä»¶ï¼ˆç·¨é›†å¯ï¼‰")
with st.form("main_form", clear_on_submit=False):
    edited_main = st.data_editor(
        disp[display_order],
        key="main_editor",
        use_container_width=True,
        hide_index=True,
        column_config=column_config_main,
        disabled=[c for c in display_order if c != "â˜…"],  # â˜…ã®ã¿ç·¨é›†å¯
        height=520,
        num_rows="fixed",
    )
    submitted_main = st.form_submit_button("âœ… å¤‰æ›´ã‚’åæ˜ ï¼ˆä¸Šã®è¡¨ï¼‰", use_container_width=True)

# ä¸Šãƒ•ã‚©ãƒ¼ãƒ ã®åæ˜ ãƒ­ã‚¸ãƒƒã‚¯
if submitted_main:
    subset_ids_main = set(disp["_row_id"].tolist())
    checked_subset_main = set(edited_main.loc[edited_main["â˜…"] == True, "_row_id"].tolist())
    st.session_state.favs = (st.session_state.favs - subset_ids_main) | checked_subset_main
    st.rerun()

st.divider()

# ===== ä¸‹ï¼šãŠæ°—ã«å…¥ã‚Šä¸€è¦§ï¼ˆãƒ•ã‚©ãƒ¼ãƒ ä¸€æ‹¬åæ˜ ï¼ãƒ•ã‚£ãƒ«ã‚¿ç„¡è¦–ï¼‰ =====
st.subheader(f"â­ ãŠæ°—ã«å…¥ã‚Šä¸€è¦§ï¼ˆå¸¸è¨­ï¼‰ â€” ç¾åœ¨ {len(st.session_state.favs)} ä»¶")

fav_disp_full = df[visible_cols_full].copy()
fav_disp_full["_row_id"] = fav_disp_full.apply(make_row_id, axis=1)
fav_disp = fav_disp_full[fav_disp_full["_row_id"].isin(st.session_state.favs)].copy()

# ã‚µãƒ–è¡¨ï¼šãƒ‡ãƒ¼ã‚¿ãŒç„¡ã„å ´åˆã®æ¡ˆå†…
if fav_disp.empty:
    st.info("ãŠæ°—ã«å…¥ã‚Šã¯æœªé¸æŠžã§ã™ã€‚ä¸Šã®è¡¨ã®ã€Žâ˜…ã€ã«ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚")
else:
    fav_disp["â˜…"] = fav_disp["_row_id"].apply(lambda rid: rid in st.session_state.favs)
    fav_display_order = ["â˜…"] + [c for c in fav_disp.columns if c not in ["â˜…", "_row_id"]] + ["_row_id"]

    column_config_fav = {
        "â˜…": st.column_config.CheckboxColumn("â˜…", help="ãƒã‚§ãƒƒã‚¯ã§è§£é™¤/è¿½åŠ ", default=True, width="small"),
    }
    if "HPãƒªãƒ³ã‚¯å…ˆ" in fav_disp.columns:
        column_config_fav["HPãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("HPãƒªãƒ³ã‚¯å…ˆ", display_text="HP")
    if "PDFãƒªãƒ³ã‚¯å…ˆ" in fav_disp.columns:
        column_config_fav["PDFãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("PDFãƒªãƒ³ã‚¯å…ˆ", display_text="PDF")

    # å³ç«¯ã«ã€Œå…¨ã¦å¤–ã™ã€ãƒœã‚¿ãƒ³
    c1, c2 = st.columns([1, 5])
    with c2:
        st.write("")  # ä½ç½®èª¿æ•´
    with c1:
        if st.button("âŒ å…¨ã¦å¤–ã™", use_container_width=True):
            st.session_state.favs = set()
            st.rerun()

    with st.form("fav_form", clear_on_submit=False):
        fav_edited = st.data_editor(
            fav_disp[fav_display_order],
            key="fav_editor",
            use_container_width=True,
            hide_index=True,
            column_config=column_config_fav,
            disabled=[c for c in fav_display_order if c != "â˜…"],
            height=420,
            num_rows="fixed",
        )
        submitted_fav = st.form_submit_button("âœ… å¤‰æ›´ã‚’åæ˜ ï¼ˆä¸‹ã®è¡¨ï¼‰", use_container_width=True)

    if submitted_fav:
        subset_ids_fav = set(fav_disp["_row_id"].tolist())
        fav_checked_subset = set(fav_edited.loc[fav_edited["â˜…"] == True, "_row_id"].tolist())
        st.session_state.favs = (st.session_state.favs - subset_ids_fav) | fav_checked_subset
        st.rerun()

# ===== ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆè¡¨ç¤ºåˆ—ã®ã¿ï¼‰ =====
export_df = edited_main.drop(columns=["â˜…", "_row_id"])
st.download_button(
    "ðŸ“¥ çµžã‚Šè¾¼ã¿çµæžœã‚’CSVå‡ºåŠ›ï¼ˆè¡¨ç¤ºåˆ—ã®ã¿ï¼‰",
    data=export_df.to_csv(index=False).encode("utf-8-sig"),
    file_name=f"filtered_{time.strftime('%Y%m%d')}.csv",
    mime="text/csv",
    use_container_width=True
)